                                                  TFOD Setup 

1..    https://github.com/tensorflow/models/tree/v1.13.0      ###  Download whole Repo

2..  Model ZOO---  https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
                                             OR
                   Select directly SSD MobileNet_V1   given below  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz

3..   https://drive.google.com/file/d/12F5oGAuQg7qBM_267TCMt_rlorV-M7gf/view?usp=sharing   # Dataset & utils Download

4..   https://tzutalin.github.io/labelImg/  # Download

5..
For protobuff to py conversion
https://github.com/protocolbuffers/protobuf/releases/download/v3.11.0/protoc-3.11.0-win64.zip      #### Download 


STEP-1 

Download the following content-
Download v1.13.0 model.

Download the ssd_mobilenet_v1_coco model from the model zoo or any other model of your choice from TensorFlow model zoo.

Download Dataset & utils.

Download labelImg tool for labeling images.

Create a folder in C drive and paste all the download files.

STEP-2

Creating virtual env using conda-

conda create -n your_env_name python=3.6

STEP-3

For CPU only

pip install pillow lxml Cython contextlib2 jupyter matplotlib pandas opencv-python tensorflow==1.14.0


STEP-4 Install protobuf using conda package manager-

conda install -c anaconda protobuf

STEP-5

For Windows 
Path------(cd C:\tfodmask\models\research)

protoc object_detection/protos/*.proto --python_out=.


STEP-6
Run command

python setup.py install

STEP-7

Annotate images using Label img S/W  (WE have to select PascalVOC and xml files we have to generate for each image)


STEP-8

All files in utils folders are to be pasted in models\research 

STEP-9

python xml_to_csv.py

STEP-10

Now in research folder generate_tfrecord we have to change the classes according to the application.

STEP-11

python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record
python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record

STEP-12

In objectdetection/legacy   copy the train.py file into research folder.
Copy Your model file (SSD Mobilenet) into research folder.

STEP-13

Copy from research/object_detection/samples/config/ YOURMODEL.config file into research/training-

In research/training folder labelmap.txt file edit file with the classes of your application.

STEP-15

In research/training folder  edit yourmodel.config file 
6 changes to done :

num_classes
fine_tune_checkpoint
num_steps
update input_path and label_map_path for both train_input_reader and eval_input_reader

STEP-16

Copy deployment and nets folder from research/slim into the research folder

## Important--   If u going to follow the next step i.e. training your model in local setup it will take time .So train in google colab by following the steps in description of github and download training folder and replace with your local system training folder and run stepno.18##

STEP-17

NOW Run the following command from the research folder. This will start the training in your local system::

python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_coco.config

STEP-18
After Training

In Objectdetection folder ,file export_inference_graph.py paste it to research folder to convert ckpt to pb file for prediction.

*Replace the XXX with the last generated ckpt file inside the training folder
Run command :
python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt-XXX --output_directory inference_graph

Under research folder Inference_graph folder will be created in that frozen_inference_graph.pb 
file will be used for predictions.


STEP-19

In research folder create test_images folder for testing and put some images.

Run the jupyter notebook objectdetection tutorial and predict.

STEP-20

From webcam using opencv

#code to get the pop camera

import cv2

cap = cv2.VideoCapture(0)
with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    while True:
      ret, image_np = cap.read()
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      # Each box represents a part of the image where a particular object was detected.
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
      # Each score represent how level of confidence for each of the objects.
      # Score is shown on the result image, together with the class label.
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')
      # Actual detection.
      (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})
      # Visualization of the results of a detection.
      vis_util.visualize_boxes_and_labels_on_image_array(
          image_np,
          np.squeeze(boxes),
          np.squeeze(classes).astype(np.int32),
          np.squeeze(scores),
          category_index,
          use_normalized_coordinates=True,
          line_thickness=8)

      cv2.imshow('object detection', cv2.resize(image_np, (800,600)))
      if cv2.waitKey(25) & 0xFF == ord('q'):
        cv2.destroyAllWindows()
        break

cap.release()


********************************************************HAPPY OBJECT DETECTION ***************************************************
